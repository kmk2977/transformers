{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44564fea",
   "metadata": {
    "papermill": {
     "duration": 0.010689,
     "end_time": "2024-07-23T15:37:57.869331",
     "exception": false,
     "start_time": "2024-07-23T15:37:57.858642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Conding Transformer network from scratch [PyTorch]***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a0959",
   "metadata": {
    "papermill": {
     "duration": 0.009606,
     "end_time": "2024-07-23T15:37:57.889149",
     "exception": false,
     "start_time": "2024-07-23T15:37:57.879543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Imporitng Necessary Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab37a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:37:57.910628Z",
     "iopub.status.busy": "2024-07-23T15:37:57.910275Z",
     "iopub.status.idle": "2024-07-23T15:38:14.550584Z",
     "shell.execute_reply": "2024-07-23T15:38:14.549519Z"
    },
    "papermill": {
     "duration": 16.654635,
     "end_time": "2024-07-23T15:38:14.553697",
     "exception": false,
     "start_time": "2024-07-23T15:37:57.899062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 15:38:03.641424: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-23 15:38:03.641522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-23 15:38:03.765639: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f2a63",
   "metadata": {
    "papermill": {
     "duration": 0.009878,
     "end_time": "2024-07-23T15:38:14.574285",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.564407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Model***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d8995",
   "metadata": {
    "papermill": {
     "duration": 0.009819,
     "end_time": "2024-07-23T15:38:14.594505",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.584686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Input Embeddings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b1826a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:14.617502Z",
     "iopub.status.busy": "2024-07-23T15:38:14.616935Z",
     "iopub.status.idle": "2024-07-23T15:38:14.623132Z",
     "shell.execute_reply": "2024-07-23T15:38:14.622261Z"
    },
    "papermill": {
     "duration": 0.020162,
     "end_time": "2024-07-23T15:38:14.625054",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.604892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430d5e1",
   "metadata": {
    "papermill": {
     "duration": 0.009916,
     "end_time": "2024-07-23T15:38:14.645295",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.635379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Positional Encoding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc47b10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:14.667135Z",
     "iopub.status.busy": "2024-07-23T15:38:14.666776Z",
     "iopub.status.idle": "2024-07-23T15:38:14.677126Z",
     "shell.execute_reply": "2024-07-23T15:38:14.676090Z"
    },
    "papermill": {
     "duration": 0.023116,
     "end_time": "2024-07-23T15:38:14.679012",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.655896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94ff19",
   "metadata": {
    "papermill": {
     "duration": 0.009867,
     "end_time": "2024-07-23T15:38:14.698770",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.688903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Layer Normalization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab13137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:14.720305Z",
     "iopub.status.busy": "2024-07-23T15:38:14.719703Z",
     "iopub.status.idle": "2024-07-23T15:38:14.728390Z",
     "shell.execute_reply": "2024-07-23T15:38:14.726786Z"
    },
    "papermill": {
     "duration": 0.0215,
     "end_time": "2024-07-23T15:38:14.730326",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.708826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66249f9",
   "metadata": {
    "papermill": {
     "duration": 0.009829,
     "end_time": "2024-07-23T15:38:14.750016",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.740187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Feed Forward Network***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ead4ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:14.771545Z",
     "iopub.status.busy": "2024-07-23T15:38:14.771285Z",
     "iopub.status.idle": "2024-07-23T15:38:14.777122Z",
     "shell.execute_reply": "2024-07-23T15:38:14.776278Z"
    },
    "papermill": {
     "duration": 0.019515,
     "end_time": "2024-07-23T15:38:14.779951",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.760436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd5325",
   "metadata": {
    "papermill": {
     "duration": 0.009801,
     "end_time": "2024-07-23T15:38:14.802888",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.793087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Multi Head Attention***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c0904b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:14.824569Z",
     "iopub.status.busy": "2024-07-23T15:38:14.823884Z",
     "iopub.status.idle": "2024-07-23T15:38:14.841929Z",
     "shell.execute_reply": "2024-07-23T15:38:14.841121Z"
    },
    "papermill": {
     "duration": 0.030909,
     "end_time": "2024-07-23T15:38:14.843795",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.812886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc36a3e",
   "metadata": {
    "papermill": {
     "duration": 0.009953,
     "end_time": "2024-07-23T15:38:14.863655",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.853702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Residual Connection***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1451f3fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:14.884869Z",
     "iopub.status.busy": "2024-07-23T15:38:14.884540Z",
     "iopub.status.idle": "2024-07-23T15:38:14.890685Z",
     "shell.execute_reply": "2024-07-23T15:38:14.889888Z"
    },
    "papermill": {
     "duration": 0.018906,
     "end_time": "2024-07-23T15:38:14.892529",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.873623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804bb101",
   "metadata": {
    "papermill": {
     "duration": 0.009844,
     "end_time": "2024-07-23T15:38:14.912279",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.902435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Encoder and EncoderBlock***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2b0d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:14.933862Z",
     "iopub.status.busy": "2024-07-23T15:38:14.933238Z",
     "iopub.status.idle": "2024-07-23T15:38:14.941095Z",
     "shell.execute_reply": "2024-07-23T15:38:14.940244Z"
    },
    "papermill": {
     "duration": 0.020719,
     "end_time": "2024-07-23T15:38:14.942974",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.922255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93885f3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:14.964235Z",
     "iopub.status.busy": "2024-07-23T15:38:14.963714Z",
     "iopub.status.idle": "2024-07-23T15:38:14.969233Z",
     "shell.execute_reply": "2024-07-23T15:38:14.968398Z"
    },
    "papermill": {
     "duration": 0.018541,
     "end_time": "2024-07-23T15:38:14.971431",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.952890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db044729",
   "metadata": {
    "papermill": {
     "duration": 0.009842,
     "end_time": "2024-07-23T15:38:14.995193",
     "exception": false,
     "start_time": "2024-07-23T15:38:14.985351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Decoder and DecoderBlock***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a892df7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.017158Z",
     "iopub.status.busy": "2024-07-23T15:38:15.016404Z",
     "iopub.status.idle": "2024-07-23T15:38:15.026052Z",
     "shell.execute_reply": "2024-07-23T15:38:15.025145Z"
    },
    "papermill": {
     "duration": 0.022744,
     "end_time": "2024-07-23T15:38:15.027965",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.005221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f7262fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.049730Z",
     "iopub.status.busy": "2024-07-23T15:38:15.049379Z",
     "iopub.status.idle": "2024-07-23T15:38:15.056256Z",
     "shell.execute_reply": "2024-07-23T15:38:15.055397Z"
    },
    "papermill": {
     "duration": 0.020038,
     "end_time": "2024-07-23T15:38:15.058264",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.038226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af8442",
   "metadata": {
    "papermill": {
     "duration": 0.00989,
     "end_time": "2024-07-23T15:38:15.078333",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.068443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***ProjectionLayer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "622f1db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.100781Z",
     "iopub.status.busy": "2024-07-23T15:38:15.100530Z",
     "iopub.status.idle": "2024-07-23T15:38:15.105430Z",
     "shell.execute_reply": "2024-07-23T15:38:15.104585Z"
    },
    "papermill": {
     "duration": 0.017503,
     "end_time": "2024-07-23T15:38:15.107332",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.089829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf0723",
   "metadata": {
    "papermill": {
     "duration": 0.010102,
     "end_time": "2024-07-23T15:38:15.127254",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.117152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Transformer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "474c0ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.148158Z",
     "iopub.status.busy": "2024-07-23T15:38:15.147912Z",
     "iopub.status.idle": "2024-07-23T15:38:15.156245Z",
     "shell.execute_reply": "2024-07-23T15:38:15.155560Z"
    },
    "papermill": {
     "duration": 0.02094,
     "end_time": "2024-07-23T15:38:15.158066",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.137126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beece22f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.178702Z",
     "iopub.status.busy": "2024-07-23T15:38:15.178462Z",
     "iopub.status.idle": "2024-07-23T15:38:15.188103Z",
     "shell.execute_reply": "2024-07-23T15:38:15.187298Z"
    },
    "papermill": {
     "duration": 0.022157,
     "end_time": "2024-07-23T15:38:15.189972",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.167815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afc82f",
   "metadata": {
    "papermill": {
     "duration": 0.009882,
     "end_time": "2024-07-23T15:38:15.209787",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.199905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Dataset***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228c3da",
   "metadata": {
    "papermill": {
     "duration": 0.009696,
     "end_time": "2024-07-23T15:38:15.229365",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.219669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Tokenizer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e09fdce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.249960Z",
     "iopub.status.busy": "2024-07-23T15:38:15.249681Z",
     "iopub.status.idle": "2024-07-23T15:38:15.256157Z",
     "shell.execute_reply": "2024-07-23T15:38:15.255294Z"
    },
    "papermill": {
     "duration": 0.018873,
     "end_time": "2024-07-23T15:38:15.257987",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.239114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining Tokenizer\n",
    "def build_tokenizer(config, ds, lang):\n",
    "    \n",
    "    # Crating a file path for the tokenizer \n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    \n",
    "    # Checking if Tokenizer already exists\n",
    "    if not Path.exists(tokenizer_path): \n",
    "        \n",
    "        # If it doesn't exist, we create a new one\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n",
    "        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n",
    "        \n",
    "        # Creating a trainer for the new tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \n",
    "                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n",
    "        \n",
    "        # Training new tokenizer on sentences from the dataset and language specified \n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
    "        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n",
    "    return tokenizer # Returns the loaded tokenizer or the trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f52db6de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.279780Z",
     "iopub.status.busy": "2024-07-23T15:38:15.279530Z",
     "iopub.status.idle": "2024-07-23T15:38:15.283577Z",
     "shell.execute_reply": "2024-07-23T15:38:15.282667Z"
    },
    "papermill": {
     "duration": 0.016666,
     "end_time": "2024-07-23T15:38:15.285533",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.268867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterating through dataset to extract the original sentence and its translation \n",
    "def get_all_sentences(ds, lang):\n",
    "    for pair in ds:\n",
    "        yield pair['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a74497f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.306614Z",
     "iopub.status.busy": "2024-07-23T15:38:15.306356Z",
     "iopub.status.idle": "2024-07-23T15:38:15.316498Z",
     "shell.execute_reply": "2024-07-23T15:38:15.315434Z"
    },
    "papermill": {
     "duration": 0.022916,
     "end_time": "2024-07-23T15:38:15.318381",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.295465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    \n",
    "    # Loading the train portion of the OpusBooks dataset.\n",
    "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
    "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train') \n",
    "    \n",
    "    # Building or loading tokenizer for both the source and target languages \n",
    "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "    \n",
    "    # Splitting the dataset for training and validation \n",
    "    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n",
    "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
    "                                    \n",
    "    # Processing data with the BilingualDataset class, which we will define below\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "                                    \n",
    "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for pair in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "        \n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "    # Creating dataloaders for the training and validadion sets\n",
    "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
    "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
    "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "583deee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.339237Z",
     "iopub.status.busy": "2024-07-23T15:38:15.338800Z",
     "iopub.status.idle": "2024-07-23T15:38:15.343155Z",
     "shell.execute_reply": "2024-07-23T15:38:15.342316Z"
    },
    "papermill": {
     "duration": 0.016858,
     "end_time": "2024-07-23T15:38:15.345077",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.328219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "        return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc7d22e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.366019Z",
     "iopub.status.busy": "2024-07-23T15:38:15.365747Z",
     "iopub.status.idle": "2024-07-23T15:38:15.381108Z",
     "shell.execute_reply": "2024-07-23T15:38:15.380223Z"
    },
    "papermill": {
     "duration": 0.028187,
     "end_time": "2024-07-23T15:38:15.383037",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.354850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    \n",
    "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
    "    # 'seq_len' defines the sequence length for both languages\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        \n",
    "        # Defining special tokens by using the target language tokenizer\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "        \n",
    "    # Total number of instances in the dataset (some pairs are larger than others)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    # Using the index to retrive source and target texts\n",
    "    def __getitem__(self, index: Any) -> Any:\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "        \n",
    "        # Tokenizing source and target texts \n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "        \n",
    "        # Computing how many padding tokens need to be added to the tokenized texts \n",
    "        # Source tokens\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "        # Target tokens\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
    "        \n",
    "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
    "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "         \n",
    "        # Building the encoder input tensor by combining several elements\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "            self.sos_token, # inserting the '[SOS]' token\n",
    "            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
    "            self.eos_token, # Inserting the '[EOS]' token\n",
    "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Building the decoder input tensor by combining several elements\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token \n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        \n",
    "        )\n",
    "        \n",
    "        # Creating a label tensor, the expected output for training the model\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                self.eos_token, # Inserting the '[EOS]' token \n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "        \n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input, \n",
    "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)), \n",
    "            'label': label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b26ab689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.403679Z",
     "iopub.status.busy": "2024-07-23T15:38:15.403447Z",
     "iopub.status.idle": "2024-07-23T15:38:15.411551Z",
     "shell.execute_reply": "2024-07-23T15:38:15.410783Z"
    },
    "papermill": {
     "duration": 0.020596,
     "end_time": "2024-07-23T15:38:15.413402",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.392806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function to obtain the most probable next token\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "    \n",
    "    # Computing the output of the encoder for the source sequence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initializing the decoder input with the Start of Sentence token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "    \n",
    "    # Looping until the 'max_len', maximum length, is reached\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "            \n",
    "        # Building a mask for the decoder input\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        \n",
    "        # Calculating the output of the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "        \n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        \n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "        \n",
    "        # If the next token is an End of Sentence token, we finish the loop\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "            \n",
    "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b941f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.433948Z",
     "iopub.status.busy": "2024-07-23T15:38:15.433653Z",
     "iopub.status.idle": "2024-07-23T15:38:15.442051Z",
     "shell.execute_reply": "2024-07-23T15:38:15.441129Z"
    },
    "papermill": {
     "duration": 0.020907,
     "end_time": "2024-07-23T15:38:15.444023",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.423116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining function to evaluate the model on the validation dataset\n",
    "# num_examples = 2, two examples per run\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "    \n",
    "    console_width = 80 # Fixed witdh for printed messages\n",
    "    \n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            \n",
    "            # Ensuring that the batch_size of the validation set is 1\n",
    "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "            \n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            \n",
    "            # Retrieving source and target texts from the batch\n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0] # True translation \n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "            \n",
    "            # Printing results\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "            \n",
    "            # After two examples, we break the loop\n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a85c1cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.464790Z",
     "iopub.status.busy": "2024-07-23T15:38:15.464505Z",
     "iopub.status.idle": "2024-07-23T15:38:15.469062Z",
     "shell.execute_reply": "2024-07-23T15:38:15.468227Z"
    },
    "papermill": {
     "duration": 0.017157,
     "end_time": "2024-07-23T15:38:15.471008",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.453851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    \n",
    "    # Loading model using the 'build_transformer' function.\n",
    "    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75597686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.492165Z",
     "iopub.status.busy": "2024-07-23T15:38:15.491896Z",
     "iopub.status.idle": "2024-07-23T15:38:15.498227Z",
     "shell.execute_reply": "2024-07-23T15:38:15.497420Z"
    },
    "papermill": {
     "duration": 0.019148,
     "end_time": "2024-07-23T15:38:15.500113",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.480965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define settings for building and training the transformer model\n",
    "def get_config():\n",
    "    return{\n",
    "        'batch_size': 8,\n",
    "        'num_epochs': 20,\n",
    "        'lr': 10**-4,\n",
    "        'seq_len': 350,\n",
    "        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
    "        'lang_src': 'en',\n",
    "        'lang_tgt': 'it',\n",
    "        'model_folder': 'weights',\n",
    "        'model_basename': 'tmodel_',\n",
    "        'preload': None,\n",
    "        'tokenizer_file': 'tokenizer_{0}.json',\n",
    "        'experiment_name': 'runs/tmodel'\n",
    "    }\n",
    "    \n",
    "\n",
    "# Function to construct the path for saving and retrieving model weights\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config['model_folder'] # Extracting model folder from the config\n",
    "    model_basename = config['model_basename'] # Extracting the base name for model files\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1fcccd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.522394Z",
     "iopub.status.busy": "2024-07-23T15:38:15.522100Z",
     "iopub.status.idle": "2024-07-23T15:38:15.538062Z",
     "shell.execute_reply": "2024-07-23T15:38:15.537226Z"
    },
    "papermill": {
     "duration": 0.029536,
     "end_time": "2024-07-23T15:38:15.539974",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.510438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Setting up device to run on GPU to train faster\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device {device}\")\n",
    "    \n",
    "    # Creating model directory to store weights\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    \n",
    "    # Initializing model on the GPU using the 'get_model' function\n",
    "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    \n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "    \n",
    "    # Setting up the Adam optimizer with the specified learning rate from the '\n",
    "    # config' dictionary plus an epsilon value\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
    "    \n",
    "    # Initializing epoch and global step variables\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    # Checking if there is a pre-trained model to load\n",
    "    # If true, loads it\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename) # Loading model\n",
    "        \n",
    "        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        # Loading the optimizer state from the saved model\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        # Loading the global step state from the saved model\n",
    "        global_step = state['global_step']\n",
    "        \n",
    "    # Initializing CrossEntropyLoss function for training\n",
    "    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
    "    # We also apply label_smoothing to prevent overfitting\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
    "    \n",
    "    # Initializing training loop \n",
    "    \n",
    "    # Iterating over each epoch from the 'initial_epoch' variable up to\n",
    "    # the number of epochs informed in the config\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        \n",
    "        # Initializing an iterator over the training dataloader\n",
    "        # We also use tqdm to display a progress bar\n",
    "        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
    "        \n",
    "        # For each batch...\n",
    "        for batch in batch_iterator:\n",
    "            model.train() # Train the model\n",
    "            \n",
    "            # Loading input data and masks onto the GPU\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "            \n",
    "            # Running tensors through the Transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "            \n",
    "            # Loading the target labels onto the GPU\n",
    "            label = batch['label'].to(device)\n",
    "            \n",
    "            # Computing loss between model's output and true labels\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            \n",
    "            # Updating progress bar\n",
    "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "            \n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "            \n",
    "            # Performing backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating parameters based on the gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Clearing the gradients to prepare for the next batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1 # Updating global step count\n",
    "            \n",
    "        # We run the 'run_validation' function at the end of each epoch\n",
    "        # to evaluate model performance\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "         \n",
    "        # Saving model\n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        # Writting current model state to the 'model_filename'\n",
    "        torch.save({\n",
    "            'epoch': epoch, # Current epoch\n",
    "            'model_state_dict': model.state_dict(),# Current model state\n",
    "            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "            'global_step': global_step # Current global step \n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fef6535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T15:38:15.561044Z",
     "iopub.status.busy": "2024-07-23T15:38:15.560756Z",
     "iopub.status.idle": "2024-07-23T20:40:29.301428Z",
     "shell.execute_reply": "2024-07-23T20:40:29.300610Z"
    },
    "papermill": {
     "duration": 18133.753636,
     "end_time": "2024-07-23T20:40:29.303679",
     "exception": false,
     "start_time": "2024-07-23T15:38:15.550043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978b1d8e4d3549da9bdddb3776ccc907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc29012bf89482094241db6b0db1956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d24406f550f4f2298ab74cd6f97105a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 00: 100%|██████████| 3638/3638 [15:02<00:00,  4.03it/s, loss=6.085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The Tartar darted off, his coat-tails flying; and five minutes later rushed in again, with a dish of opened oysters in pearly shells and a bottle between his fingers.\n",
      "TARGET: E il tartaro con le falde svolazzanti, corse via e dopo cinque minuti entrò volando con un vassoio di ostriche aperte sui gusci di madreperla e una bottiglia fra le dita.\n",
      "PREDICTED: Il suo momento , e la sua , e la sua sua , e la sua sua , e la sua , e la sua .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: In the present instance, going back to the liver-pill circular, I had the symptoms, beyond all mistake, the chief among them being \"a general disinclination to work of any kind.\"\n",
      "TARGET: Nel caso presente, per ritornare all’annuncio delle pillole per il fegato, io avevo i sintomi d’una malattia di fegato, dei quali il principale era «una generale svogliatezza al lavoro di qualunque specie».\n",
      "PREDICTED: Il mio momento , e la mia , e la mia , e la mia , e la mia , e la mia .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 01: 100%|██████████| 3638/3638 [15:02<00:00,  4.03it/s, loss=6.080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: We did get the thing up at last, the two of us together. We fixed it, not exactly upside down - more sideways like - and we tied it up to the mast with the painter, which we cut off for the purpose.\n",
      "TARGET: Finalmente tutti e due insieme riuscimmo a fissarla, ma esattamente sottosopra — un po’ lateralmente — e la legammo all’albero con la gomena tagliata per quello scopo.\n",
      "PREDICTED: , come , , , , e non ci , e non si , e non , e , e , e , , , e , , , , .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Oblonsky was telling his sister-in-law the pun he had made about 'dissolving marriages.'\n",
      "TARGET: Stepan Arkad’ic raccontava alla cognata il suo giuoco di parole sul divorzio.\n",
      "PREDICTED: Stepan Arkad ’ ic era stato stato stato di nuovo , che aveva detto il suo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 02: 100%|██████████| 3638/3638 [15:01<00:00,  4.04it/s, loss=5.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I have a new bodyguard – Mary Vlasyevna,' this was the midwife, a new and important personage in the Levins' family life.\n",
      "TARGET: Ho una nuova guardia del corpo, Mar’ja Vlas’evna — era la levatrice, un personaggio nuovo, importante nella vita familiare di Levin. — È venuta a farmi visita.\n",
      "PREDICTED: Io sono un uomo di voce , che aveva detto che la sua vita era stata la vita e la vita di vita .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: A BLUSTERING STORM WAS RUSHING and whistling between the wheels of the train and round the pillars and the corners of the station.\n",
      "TARGET: Una tormenta paurosa s’era scatenata e fischiava fra le ruote della vettura, lungo le colonne, al di là dell’angolo della stazione.\n",
      "PREDICTED: Il suo , e la sua luce del governatorato , la folla del quale si e la folla di .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 03: 100%|██████████| 3638/3638 [15:04<00:00,  4.02it/s, loss=4.592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He was holding with one hand the window of a carriage (from which the head of a lady in a velvet bonnet and two little children's heads were leaning out) and was smilingly beckoning with his other hand to his brother-in-law.\n",
      "TARGET: Si teneva con una mano al finestrino di una carrozza che s’era fermata all’angolo, dalla quale si sporgevano una testa di donna con un cappello di velluto e due testoline di bimbi, e sorrideva e faceva segno con l’altra mano al cognato.\n",
      "PREDICTED: Egli si mise a guardare con la finestra , che si la testa di un ’ occhiata alla finestra e un ’ altra , che si era un ’ altra , e il dottore si era con la mano e la principessa .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The great gates were closed and locked; but a wicket in one of them was only latched.\n",
      "TARGET: Aprii la porta e la chiusi dolcemente.\n",
      "PREDICTED: Il sole era un po ’ di seta e di un vestito .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 04: 100%|██████████| 3638/3638 [15:04<00:00,  4.02it/s, loss=5.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: (He vividly pictured to himself Mlle Roland's roguish black eyes, and her smile.) 'Besides, as long as she was in the house I never took any liberties.\n",
      "TARGET: Ma che governante! — e ricordò con vivezza il riso e gli occhi neri assassini di m.lle Rolland. — Del resto finché è stata in casa nostra, io non mi sono permesso nulla.\n",
      "PREDICTED: ( con un sorriso di gioia , con gli occhi scintillanti , le lacrime , e la njanja , come egli aveva detto , con la casa , non era mai più nulla di più .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Laska! Here!' he said, pointing to the other side.\n",
      "TARGET: “Laska, qua!” egli disse, indicandole l’altra parte.\n",
      "PREDICTED: — Laska ! — disse , indicando il loro posto .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 05: 100%|██████████| 3638/3638 [15:04<00:00,  4.02it/s, loss=5.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Oh, this is a shame!\" And he flung the watch down, and sprang out of bed, and had a cold bath, and washed himself, and dressed himself, and shaved himself in cold water because there was not time to wait for the hot, and then rushed and had another look at the watch.\n",
      "TARGET: Vergogna! — E scagliò lontano l’orologio, saltò dal letto, fece un bagno freddo, si lavò, si vestì, si fece la barba con l’acqua fredda, perchè non vi era tempo d’aspettare la calda, e poi si precipitò a dare un’altra occhiata all’orologio.\n",
      "PREDICTED: Oh ! questa è una donna ! — gridò il Coniglio , e , , si alzò e , , si mise a guardare un lume , e si mise a guardare un ' altra volta , e poi non si poteva .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I proceeded: at last my way opened, the trees thinned a little; presently I beheld a railing, then the house--scarce, by this dim light, distinguishable from the trees; so dank and green were its decaying walls.\n",
      "TARGET: Presto scorsi una cancellata, poi una casa. L'oscurità impediva quasi di distinguerla dagli alberi, tanto le mura erano scure, umide e verdastre.\n",
      "PREDICTED: Mi alzai e mi sedei sul fuoco , e mi sentii un ' altra volta , e la casa mi pareva che di sotto il cielo e il cielo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 06: 100%|██████████| 3638/3638 [15:05<00:00,  4.02it/s, loss=4.535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'Enoch, Enos...'\n",
      "TARGET: — Enoch, Enos.\n",
      "PREDICTED: — Tutto , un ...\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: She gave the order to harness another pair of horses, and busied herself packing her handbag with things necessary for a few days.\n",
      "TARGET: Ordinò di attaccare altri cavalli e si occupò di mettere in una sacca da viaggio le cose indispensabili per qualche giorno.\n",
      "PREDICTED: Ella aveva mandato a prendere un altro di cavalli e , il denaro con la strada , in qualche tempo per la giornata .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 07: 100%|██████████| 3638/3638 [15:01<00:00,  4.03it/s, loss=4.057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I hate him: and I cannot forgive myself.'\n",
      "TARGET: — Lo odio, e non riesco a perdonarmelo.\n",
      "PREDICTED: — Io lo odio , e non posso perdonare .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Of Mr. Reed's ghost I am: he died in that room, and was laid out there.\n",
      "TARGET: — Sì, ho paura dell'ombra del signor Reed, che morì in quella camera, e di là fu portato a sotterrare.\n",
      "PREDICTED: — Ebbene , il signor Reed è morto ; è morto e là ci sono .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 08: 100%|██████████| 3638/3638 [15:01<00:00,  4.03it/s, loss=3.926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Oblonsky during the drive was composing the menu of their dinner.\n",
      "TARGET: Stepan Arkad’ic durante il percorso componeva la lista del pranzo.\n",
      "PREDICTED: Stepan Arkad ’ ic si le , a pranzo .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I had a short jacket of goat’s skin, the skirts coming down to about the middle of the thighs, and a pair of open-kneed breeches of the same; the breeches were made of the skin of an old he-goat, whose hair hung down such a length on either side that, like pantaloons, it reached to the middle of my legs; stockings and shoes I had none, but had made me a pair of somethings, I scarce knew what to call them, like buskins, to flap over my legs, and lace on either side like spatterdashes, but of a most barbarous shape, as indeed were all the rest of my clothes.\n",
      "TARGET: Il mio abito era una specie di saio di pelle di capra anch’esso, i cui lembi mi venivano giù sino alla coscia, ed un paio di brache aperte al ginocchio della medesima pelle, che per altro appartenne ad un vecchio caprone, il cui pelo mi scendea da entrambi i lati sino a mezza gamba formandomi una specie di pantaloni; calze, scarpe io non ne avea di veruna sorta; nondimeno io m’avea fatto un paio di cose, che non so come nominare: chiamiamole borzacchini, che coprendomi il resto della gamba, si allacciavano da una parte come le uose; ma d’una barbarissima forma come, per dir la verità, era di barbarissima forma tutto il restante del mio abbigliamento.\n",
      "PREDICTED: un grosso di pelle di capra , di a le , a le di esse , a di , di là da me ne trovai un piccolo di pelle di pelle di pelle di capra ; ma non sapeva come fosse possibile , nè mi a tal punto di scarpe , e come i miei occhi , e le mie , mi trovai come mi a il mio , e mi per altro , perchè non aveva altro mi un gran di , come un di carne , come un gran per le mie , e il terreno per me ne .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 09: 100%|██████████| 3638/3638 [15:01<00:00,  4.03it/s, loss=4.782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I call that downright wisdom, not merely as regards the present case, but with reference to our trip up the river of life, generally.\n",
      "TARGET: Io la dichiaro, questa, vera saggezza, non semplicemente rispetto al nostro caso particolare, ma al nostro pellegrinaggio sul fiume della vita, in generale.\n",
      "PREDICTED: Io considero che questa mancanza di non essere il diritto , ma che ci sia la nostra vita , come ci siamo per lavorare i nostri , come quelli che ci sia .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'I know, I know,' he said with a smile. 'I am a family man myself.\n",
      "TARGET: — Lo so, lo so — disse il dottore, sorridendo — io stesso ho famiglia; ma noi mariti, in questi momenti, siamo le persone più pietose.\n",
      "PREDICTED: — Io so , so — disse lui sorridendo . — Sono un uomo di famiglia .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 10: 100%|██████████| 3638/3638 [15:01<00:00,  4.04it/s, loss=3.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: During the sacrament Levin did that which, agnostic though he was, he had done a thousand times before.\n",
      "TARGET: Durante la funzione Levin pregava e faceva proprio quello che lui, miscredente, aveva fatto mille volte.\n",
      "PREDICTED: Levin , soprattutto , che Levin non amava , era stato a volte un ’ altra volta , e che era già .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Here is to your health, ministrant spirit!\" he said.\n",
      "TARGET: — Alla vostra salute, spirito benefico!\n",
      "PREDICTED: — Qui è la vostra salute , ! — disse .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 11: 100%|██████████| 3638/3638 [15:01<00:00,  4.04it/s, loss=3.514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"So could I--with a roast onion.\n",
      "TARGET: — Anch'io, con le cipolle arrostite.\n",
      "PREDICTED: — Sì , potrei , un .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I said it was the sort of thing I had often longed for myself; and we discussed the possibility of our going away, we four, to some handy, well-fitted desert island, and living there in the woods.\n",
      "TARGET: Io osservai che era proprio quello che avevo sempre desiderato per me; e noi discutemmo la possibilità di andarcene, noi quattro, in qualche bell’isola deserta, a vivere nei boschi.\n",
      "PREDICTED: Dissi allora che mi era parso di aver tanto paura di veder la possibilità di fare la nostra vita ; e ci a remare , per qualche distanza di distanza , e per altro che in distanza le foreste .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 12: 100%|██████████| 3638/3638 [15:03<00:00,  4.03it/s, loss=3.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He knew that between him and her there could and should be nothing secret, and therefore he decided that it was his duty, but he had not considered how the confession might affect her: he had not put himself in her place.\n",
      "TARGET: Sapeva che fra di loro non potevano e non dovevano esserci segreti e perciò aveva deciso di far così, ma non si era reso conto degli effetti che ne sarebbero potuti derivare, non si era trasferito in lei.\n",
      "PREDICTED: Sapeva che l ’ avrebbe trovato fra lei e lei e l ’ unico segreto , e perciò decise di non essere sicuro , ma che , pensando , non aveva pensato a quanto avrebbe potuto , avrebbe potuto in un posto .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: What a consternation of soul was mine that dreary afternoon!\n",
      "TARGET: Quale costernazione erasi insinuata nell'anima mia in quel triste pomeriggio!\n",
      "PREDICTED: Che orrore !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 13: 100%|██████████| 3638/3638 [15:03<00:00,  4.03it/s, loss=2.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: But I am in a hot climate, where, if I had clothes, I could hardly wear them.\n",
      "TARGET: Ma sono in un clima caldo, e ancorchè avessi vesti, non potrei comportarle. Io sono senza difesa, e senza mezzi di contrastare alla violenza degli uomini o delle bestie.\n",
      "PREDICTED: Ma sono in una piccola striscia di fuoco , quando mi son caduta , se non avessi bevuto di trenta .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: She knew his habit, which had become a necessity, of reading in the evening.\n",
      "TARGET: Conosceva la sua abitudine, che era ormai una necessità, di leggere la sera.\n",
      "PREDICTED: Capì , ora , con quel che era stato fatto per la necessità di leggere .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 14: 100%|██████████| 3638/3638 [15:03<00:00,  4.03it/s, loss=2.720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I showed him the volume on the shelf: he took it down, and withdrawing to his accustomed window recess, he began to read it.\n",
      "TARGET: Un momento dopo era al solito, nel vano della finestra a leggere.\n",
      "PREDICTED: Lo feci in giro per il muro e lo feci in giro , quando gli feci posto in piedi , cominciò a leggere .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Isn't he a fine fellow?\n",
      "TARGET: Non è vero che è un gran bravo ragazzo?\n",
      "PREDICTED: Non è vero , amico ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 15: 100%|██████████| 3638/3638 [15:03<00:00,  4.02it/s, loss=3.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Distrust it, sir; it is not a true angel.\"\n",
      "TARGET: — Diffidate di lui, non è un angiolo vero.\n",
      "PREDICTED: — Quello che signore , non è vero , non è vero .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I am glad to be so near you again.\"\n",
      "TARGET: Sono felice di essere accanto a voi. — Jane Eyre!\n",
      "PREDICTED: Sono contenta che siate venuta a piedi .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 16: 100%|██████████| 3638/3638 [15:04<00:00,  4.02it/s, loss=1.969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: This was the first time that I entertained a thought of breeding up some tame creatures, that I might have food when my powder and shot was all spent.\n",
      "TARGET: Fu questa la prima volta che mi nacque il pensiere di addimesticare animali, per ritrarne nudrimento quando la mia polvere e le mie munizioni sarebbero finite del tutto.\n",
      "PREDICTED: Fu questo il primo pensiere che io avessi pensato a ciò che mi avrebbe potuto scoprire una certa quantità di costoro , che mi avrebbe dato a gran voglia di dar fuoco e alle mie munizioni .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: He knew that between him and her there could and should be nothing secret, and therefore he decided that it was his duty, but he had not considered how the confession might affect her: he had not put himself in her place.\n",
      "TARGET: Sapeva che fra di loro non potevano e non dovevano esserci segreti e perciò aveva deciso di far così, ma non si era reso conto degli effetti che ne sarebbero potuti derivare, non si era trasferito in lei.\n",
      "PREDICTED: Sapeva che gli si erano e che ci avrebbe potuto trovare un segreto , e perciò decise di non aver avuto la propria opinione , ma come egli sapeva , non come egli avesse potuto questa opinione .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 17: 100%|██████████| 3638/3638 [15:04<00:00,  4.02it/s, loss=2.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: If you ask me – the betrothal to-day and the wedding to-morrow!'\n",
      "TARGET: Se domandate a me, per me, oggi la benedizione e domani le nozze.\n",
      "PREDICTED: Se vuoi fare la pace , domani se domani al matrimonio ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I laughed at him as he said this.\n",
      "TARGET: Risi nel sentirlo parlar così.\n",
      "PREDICTED: Gli feci ridere come egli mi disse .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 18: 100%|██████████| 3638/3638 [15:04<00:00,  4.02it/s, loss=2.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: 'No, we are very comfortable here,' replied the ambassador's wife smiling, and she continued the interrupted conversation.\n",
      "TARGET: — No, stiamo tanto bene qui — rispose con un sorriso la moglie dell’ambasciatore, e riprese la conversazione di poco prima.\n",
      "PREDICTED: — No , ci siamo molto contenti — rispose l ’ offesa , e , continuò a parlare del discorso .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: This is you, who have been as slippery as an eel this last month, and as thorny as a briar-rose?\n",
      "TARGET: Perché da un mese mi sgusciate di mano come un'anguilla e pungete come un cespuglio di rose?\n",
      "PREDICTED: Che è , che siete una , come un lavoro d ’ un mese ? Un , come un albero che non si possa avvicinarsi ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 19: 100%|██████████| 3638/3638 [15:04<00:00,  4.02it/s, loss=2.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: My husband! Ah, yes...\n",
      "TARGET: Mio marito, ah, già....\n",
      "PREDICTED: Ma se mio marito ...\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Only the invalid himself did not show that desire, but on the contrary was angry because the doctor had not been fetched, and he continued taking medicine and talking of life.\n",
      "TARGET: Soltanto il malato non esprimeva questo sentimento, al contrario, si irritava perché non gli portavano il dottore, e continuava a prendere la medicina e parlava di vivere.\n",
      "PREDICTED: Solo il malato non aveva il desiderio di lasciarsi prendere , ma al contrario , al dottore , non era stato portato , e , la vita , si mise a parlare e si era fatto a parlare della vita .\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings('ignore') # Filtering warnings\n",
    "    config = get_config() # Retrieving config settings\n",
    "    train_model(config) # Training model with the config arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81806a",
   "metadata": {
    "papermill": {
     "duration": 11.935715,
     "end_time": "2024-07-23T20:40:53.152192",
     "exception": false,
     "start_time": "2024-07-23T20:40:41.216477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18193.609883,
   "end_time": "2024-07-23T20:41:08.712386",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-23T15:37:55.102503",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0159acefeb964db6841d8550da44284d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d24406f550f4f2298ab74cd6f97105a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_954e32141d7945b0963da795ff864bd8",
        "IPY_MODEL_b15fd20cebe548af8f8d20896361fd67",
        "IPY_MODEL_db0f37ce2932465a9de6b64cb9d866aa"
       ],
       "layout": "IPY_MODEL_a1f9373a04dd4f35abc39d7668e46866"
      }
     },
     "1dd700439e0541dca1555b31547a6497": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2485d169d54e4105801c9aaf02d5af31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "305b9cabb3114752967b7f925def6f3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32b5597fecf1441d8a1faff5fee9f302": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36fabda75dd54ac082a4361bb782f509": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3a18e727069a47cdbd80c86c29671f6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3aab4cbb43f64497a97d3bdeb44d0062": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "41af99741c7a4b3381dbf037f63ea5e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4d3a6d4e647f4c39b279d25962375b68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9ccdbe3858b34298b99ec09d0b1d63aa",
       "placeholder": "​",
       "style": "IPY_MODEL_b8a7ca198bb14409a9a4c1d8703b4eb2",
       "value": " 28.1k/28.1k [00:00&lt;00:00, 2.18MB/s]"
      }
     },
     "4dfb92e3bac34814a4f87e1912bbb976": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_80e19bcd790148898532b2b1f184cffe",
       "placeholder": "​",
       "style": "IPY_MODEL_3aab4cbb43f64497a97d3bdeb44d0062",
       "value": " 1/1 [00:00&lt;00:00, 103.44it/s]"
      }
     },
     "58bdf552240144178baf001d41dd76df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b8859032c6d84b5db07025f34af45a11",
       "placeholder": "​",
       "style": "IPY_MODEL_cdf6a12dfd364290bae8be7bed8ec840",
       "value": " 5.73M/5.73M [00:13&lt;00:00, 426kB/s]"
      }
     },
     "629fc2cc33494b50be8938d9bd6888cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "773426ff79854279913b3e639c98579f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "786464c69dba4ddc9f4860f9547121b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1dd700439e0541dca1555b31547a6497",
       "placeholder": "​",
       "style": "IPY_MODEL_e0d99db035e4482ebbd4e05e10c82b66",
       "value": "Downloading data: 100%"
      }
     },
     "7a0e51eaa7524743b0f143265db5143e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_629fc2cc33494b50be8938d9bd6888cf",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ea5cea89e77643258bbed56f4545f94a",
       "value": 1.0
      }
     },
     "80e19bcd790148898532b2b1f184cffe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8bc29012bf89482094241db6b0db1956": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_786464c69dba4ddc9f4860f9547121b7",
        "IPY_MODEL_ea25ee75f32d417781010a341ec975b5",
        "IPY_MODEL_58bdf552240144178baf001d41dd76df"
       ],
       "layout": "IPY_MODEL_32b5597fecf1441d8a1faff5fee9f302"
      }
     },
     "934658d8331844fcbf5af5df9513b14a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_efb2d42be1214420b0698744c9a5191b",
       "placeholder": "​",
       "style": "IPY_MODEL_e868438c55cc48e9a708ae7e5e3326b0",
       "value": "Computing checksums: 100%"
      }
     },
     "954e32141d7945b0963da795ff864bd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a5f71048a40b415887fc1ab81d45fa21",
       "placeholder": "​",
       "style": "IPY_MODEL_aeb6808cc5b448f3bf54a39bf1ef1f7d",
       "value": "Generating train split: 100%"
      }
     },
     "978b1d8e4d3549da9bdddb3776ccc907": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f486fdd7910f4c3f9c11fc6361a135ee",
        "IPY_MODEL_f8535c54c800423abb6b529b2e18f1ae",
        "IPY_MODEL_4d3a6d4e647f4c39b279d25962375b68"
       ],
       "layout": "IPY_MODEL_aeb5ab520afb44868fbd62795e5fc19a"
      }
     },
     "9ccdbe3858b34298b99ec09d0b1d63aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1f9373a04dd4f35abc39d7668e46866": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5f71048a40b415887fc1ab81d45fa21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a664545f8f614e209088887a5329ca51": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ae92552763444d3eb9c2f41767794a8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_934658d8331844fcbf5af5df9513b14a",
        "IPY_MODEL_7a0e51eaa7524743b0f143265db5143e",
        "IPY_MODEL_4dfb92e3bac34814a4f87e1912bbb976"
       ],
       "layout": "IPY_MODEL_0159acefeb964db6841d8550da44284d"
      }
     },
     "aeb5ab520afb44868fbd62795e5fc19a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aeb6808cc5b448f3bf54a39bf1ef1f7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b15fd20cebe548af8f8d20896361fd67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a664545f8f614e209088887a5329ca51",
       "max": 32332.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3a18e727069a47cdbd80c86c29671f6d",
       "value": 32332.0
      }
     },
     "b8859032c6d84b5db07025f34af45a11": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8a7ca198bb14409a9a4c1d8703b4eb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c775ac92fb8640caa47e9640659d1363": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cdf6a12dfd364290bae8be7bed8ec840": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "db0f37ce2932465a9de6b64cb9d866aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_305b9cabb3114752967b7f925def6f3a",
       "placeholder": "​",
       "style": "IPY_MODEL_2485d169d54e4105801c9aaf02d5af31",
       "value": " 32332/32332 [00:00&lt;00:00, 300907.40 examples/s]"
      }
     },
     "e0d99db035e4482ebbd4e05e10c82b66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e0eecf225fdb4183a2fc3010f9a025d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e868438c55cc48e9a708ae7e5e3326b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ea25ee75f32d417781010a341ec975b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_773426ff79854279913b3e639c98579f",
       "max": 5726189.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_36fabda75dd54ac082a4361bb782f509",
       "value": 5726189.0
      }
     },
     "ea5cea89e77643258bbed56f4545f94a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "efb2d42be1214420b0698744c9a5191b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f486fdd7910f4c3f9c11fc6361a135ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e0eecf225fdb4183a2fc3010f9a025d0",
       "placeholder": "​",
       "style": "IPY_MODEL_41af99741c7a4b3381dbf037f63ea5e4",
       "value": "Downloading readme: 100%"
      }
     },
     "f5bfd08936474bcb9370a61904fe8af1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f8535c54c800423abb6b529b2e18f1ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c775ac92fb8640caa47e9640659d1363",
       "max": 28064.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f5bfd08936474bcb9370a61904fe8af1",
       "value": 28064.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
